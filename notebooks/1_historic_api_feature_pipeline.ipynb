{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill and Feature Engineering Notebook\n",
    "This notebook consists of 5 parts:\n",
    "1. Importing libaries and loading packages\n",
    "2. Data Loading\n",
    "3. Data Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Hopsworks Feature Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, our decision-making process is informed by insights gained from the exploratory data analysis (EDA) we conducted. This analysis helped us identify the most relevant information for our methods and strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libaries and loading packages\n",
    "In this section, we import necessary libraries and define key functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Package for hopsworks integration\n",
    "# !pip install -U hopsworks --quiet\n",
    "\n",
    "# Import standard Python libraries\n",
    "import pandas as pd \n",
    "import hopsworks \n",
    "import numpy as np \n",
    "\n",
    "# Import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn.metrics import silhouette_score  \n",
    "\n",
    "# Import other useful libraries\n",
    "import uuid  # Unique identifier generation\n",
    "import requests  # For making API requests\n",
    "import json  \n",
    "import io \n",
    "import os\n",
    "import base64 \n",
    "from datetime import datetime, timedelta  # Date/time handling and manipulation\n",
    "import pytz  # Timezone conversions and support\n",
    "\n",
    "# Environment variable management\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "We load historic data to be used in the notebook.\n",
    "\n",
    "- The data is gathered trough an API given by the company, and the API data results are stored in the *bikelane_historic_data.csv* and *building_historic_data.csv*. The data loacated in the *data* folder is old data that the EDA and end-to-end-pipeline is made from. Here you can also find more information about the data in the readme.txt file.\n",
    "\n",
    "- The data derivied from the API is from two parking spots, one of the spots are close to a building and the other close to a bikelane, and will be refered to with this as the identifyer.\n",
    "\n",
    "- In this section we will be pinging the API and saving historic data from march and april in two csv-files, containing data from the parking spot close to the building and the one close to the bikelane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timezone object for GMT+2\n",
    "timezone = pytz.timezone('Europe/Copenhagen')\n",
    "now = datetime.now(timezone)  # Get current time \n",
    "today = now \n",
    "yesterday = today - timedelta(days=1)\n",
    "tomorrow = today + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format 'today', 'tomorrow', and 'yesterday' as \"YYYY-MM-DD\"\n",
    "formatted_today = today.strftime('%Y-%m-%d %H:%M:%S')\n",
    "formatted_tomorrow = tomorrow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "formatted_yesterday = yesterday.strftime('%Y-%m-%d %H:%M:%S')\n",
    "dev_eui_building = \"0080E115003BEA91\"\n",
    "dev_eui_bikelane = \"0080E115003E3597\"\n",
    "url = \"https://data.sensade.com\"\n",
    "username = \"ajakup20@student.aau.dk\"\n",
    "\n",
    "basic_auth = base64.b64encode(f\"{username}:{os.getenv('API_PASSWORD')}\".encode())\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Basic {basic_auth.decode(\"utf-8\")}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ping the API and get data in a given time interval\n",
    "def API_call(dev_eui, from_date, to_date):\n",
    "    payload = json.dumps({\n",
    "    \"dev_eui\": dev_eui,\n",
    "    \"from\": from_date,\n",
    "    \"to\": to_date\n",
    "})\n",
    "\n",
    "    API_response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if API_response.status_code != 200:\n",
    "        exit(13)\n",
    "\n",
    "    csv_data = API_response.text\n",
    "    df = pd.read_csv(io.StringIO(csv_data))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section below is commented out because the files are already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# march_1_building = API_call(dev_eui_building, \"2024-03-01\", \"2024-03-14\")\n",
    "# march_2_building = API_call(dev_eui_building, \"2024-03-15\", \"2024-03-31\")\n",
    "# april_1_building = API_call(dev_eui_building, \"2024-04-01\", \"2024-04-14\")\n",
    "# april_2_building = API_call(dev_eui_building, \"2024-04-15\", \"2024-04-30\")# \n",
    "\n",
    "# march_1_bikelane = API_call(dev_eui_bikelane, \"2024-03-01\", \"2024-03-14\")\n",
    "# march_2_bikelane = API_call(dev_eui_bikelane, \"2024-03-15\", \"2024-03-31\")\n",
    "# april_1_bikelane = API_call(dev_eui_bikelane, \"2024-04-01\", \"2024-04-14\")\n",
    "# april_2_bikelane = API_call(dev_eui_bikelane, \"2024-04-15\", \"2024-04-30\")\n",
    "\n",
    "# building_historic_df = pd.concat([march_1_building, march_2_building, april_1_building, april_2_building], ignore_index=True)\n",
    "# bikelane_historic_df = pd.concat([march_1_bikelane, march_2_bikelane, april_1_bikelane, april_2_bikelane], ignore_index=True)\n",
    "\n",
    "# saving the data as CSV\n",
    "# building_historic_df.to_csv('building_historic_data.csv', index=False)\n",
    "# bikelane_historic_df.to_csv('bikelane_historic_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data saved in the directory\n",
    "building_historic_df = pd.read_csv('building_historic_data.csv')\n",
    "bikelane_historic_df = pd.read_csv('bikelane_historic_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "Now before just dumping the data into a feature store we do a little preprocessing to enhance the use of our datasets.\n",
    "\n",
    "This preprocessing consists of:\n",
    "- Making unique identifyers for each datapoint\n",
    "- Combining the three datasets into one \n",
    "- Making clusters used for labeling, which is nessesary when we want to train our models later\n",
    "- Converting the data column to pandas datetime\n",
    "- minor adjustments for the naming of radar columns to fix some hopsworks problem where the name of the columns cannot start with a number, and making the relevant columns into float format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "weather_params = {\n",
    "\t\"latitude\": 57.01,\n",
    "\t\"longitude\": 9.99,\n",
    "\t\"start_date\": \"2024-03-01\",\n",
    "\t\"end_date\": \"2024-04-30\",\n",
    "\t\"hourly\": [\"temperature_2m\", \"precipitation\"],\n",
    "\t\"timezone\": \"Europe/Berlin\"\n",
    "}\n",
    "responses = openmeteo.weather_api(weather_url, params=weather_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 56.977149963378906°N 10.0632905960083°E\n",
      "Elevation 23.0 m asl\n",
      "Timezone b'Europe/Berlin' b'CEST'\n",
      "Timezone difference to GMT+0 7200 s\n",
      "                          date  temperature_2m  precipitation\n",
      "0    2024-02-29 22:00:00+00:00        6.480500            0.9\n",
      "1    2024-02-29 23:00:00+00:00        6.230500            0.6\n",
      "2    2024-03-01 00:00:00+00:00        6.230500            0.1\n",
      "3    2024-03-01 01:00:00+00:00        5.730500            0.1\n",
      "4    2024-03-01 02:00:00+00:00        5.230500            0.0\n",
      "...                        ...             ...            ...\n",
      "1459 2024-04-30 17:00:00+00:00       15.080501            0.0\n",
      "1460 2024-04-30 18:00:00+00:00       14.180500            0.0\n",
      "1461 2024-04-30 19:00:00+00:00       13.330501            0.0\n",
      "1462 2024-04-30 20:00:00+00:00       13.130500            0.0\n",
      "1463 2024-04-30 21:00:00+00:00       12.980500            0.0\n",
      "\n",
      "[1464 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(1).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "print(hourly_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the timezone from the date column\n",
    "hourly_dataframe['date'] = hourly_dataframe['date'].dt.tz_localize(None)\n",
    "#Convert to datetime object\n",
    "hourly_dataframe['date'] = pd.to_datetime(hourly_dataframe['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_dataframe['date'] = hourly_dataframe['date'] + timedelta(hours=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'time' column to datetime, coercing errors to NaT\n",
    "building_historic_df['time'] = pd.to_datetime(building_historic_df['time'], errors='coerce')\n",
    "bikelane_historic_df['time'] = pd.to_datetime(bikelane_historic_df['time'], errors='coerce')\n",
    "# Remove rows where the 'time' column is NaT\n",
    "building_historic_df = building_historic_df.dropna(subset=['time'])\n",
    "bikelane_historic_df = bikelane_historic_df.dropna(subset=['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column for the time in the format of \"YYYY-MM-DD HH\" to merge with weather data\n",
    "bikelane_historic_df['time_hour'] = bikelane_historic_df['time'].dt.strftime('%Y-%m-%d %H')\n",
    "building_historic_df['time_hour'] = building_historic_df['time'].dt.strftime('%Y-%m-%d %H')\n",
    "# Converting the time_hour column to datetime\n",
    "bikelane_historic_df['time_hour'] = pd.to_datetime(bikelane_historic_df['time_hour'])\n",
    "building_historic_df['time_hour'] = pd.to_datetime(building_historic_df['time_hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the weather data with the building sensor data\n",
    "building_historic_df = pd.merge(building_historic_df, hourly_dataframe, left_on='time_hour', right_on='date', how='left')\n",
    "# Merging the weather data with the bikelane sensor data\n",
    "bikelane_historic_df = pd.merge(bikelane_historic_df, hourly_dataframe, left_on='time_hour', right_on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backfill missing values in radar columns and battery column with the previous value\n",
    "building_historic_df['0_radar'] = building_historic_df['0_radar'].bfill()\n",
    "building_historic_df['1_radar'] = building_historic_df['1_radar'].bfill()\n",
    "building_historic_df['2_radar'] = building_historic_df['2_radar'].bfill()\n",
    "building_historic_df['3_radar'] = building_historic_df['3_radar'].bfill()\n",
    "building_historic_df['4_radar'] = building_historic_df['4_radar'].bfill()\n",
    "building_historic_df['5_radar'] = building_historic_df['5_radar'].bfill()\n",
    "building_historic_df['6_radar'] = building_historic_df['6_radar'].bfill()\n",
    "building_historic_df['7_radar'] = building_historic_df['7_radar'].bfill()\n",
    "building_historic_df['battery'] = building_historic_df['battery'].bfill()\n",
    "bikelane_historic_df['0_radar'] = bikelane_historic_df['0_radar'].bfill()\n",
    "bikelane_historic_df['1_radar'] = bikelane_historic_df['1_radar'].bfill()\n",
    "bikelane_historic_df['2_radar'] = bikelane_historic_df['2_radar'].bfill()\n",
    "bikelane_historic_df['3_radar'] = bikelane_historic_df['3_radar'].bfill()\n",
    "bikelane_historic_df['4_radar'] = bikelane_historic_df['4_radar'].bfill()\n",
    "bikelane_historic_df['5_radar'] = bikelane_historic_df['5_radar'].bfill()\n",
    "bikelane_historic_df['6_radar'] = bikelane_historic_df['6_radar'].bfill()\n",
    "bikelane_historic_df['7_radar'] = bikelane_historic_df['7_radar'].bfill()\n",
    "bikelane_historic_df['battery'] = bikelane_historic_df['battery'].bfill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing date column\n",
    "building_historic_df = building_historic_df.drop(columns=['date'])\n",
    "bikelane_historic_df = bikelane_historic_df.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique identifier for each row in the datasets\n",
    "def create_id(df, dataset_name):\n",
    "    # Assign the sensor prefix based on the dataset name\n",
    "    if dataset_name == 'building_historic_df':\n",
    "        df['psensor'] = \"BUILDING\"\n",
    "    elif dataset_name == 'bikelane_historic_df':\n",
    "        df['psensor'] = \"BIKELANE\"\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name provided\")\n",
    "\n",
    "    # Create a new column 'id' with a unique identifier for each row\n",
    "    df['id'] = df['time'].astype(str) + '_' + df['psensor']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the ID creator function to the datasets\n",
    "df_bikelane = create_id(bikelane_historic_df, 'bikelane_historic_df')\n",
    "df_building = create_id(building_historic_df, 'building_historic_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the radar columns to start with radar to deal with hopsworks problem\n",
    "df_building = df_building.rename(columns={'0_radar': 'radar_0', '1_radar': 'radar_1', '2_radar': 'radar_2', '3_radar': 'radar_3', '4_radar': 'radar_4', '5_radar': 'radar_5', '6_radar': 'radar_6', '7_radar': 'radar_7'})\n",
    "df_bikelane = df_bikelane.rename(columns={'0_radar': 'radar_0', '1_radar': 'radar_1', '2_radar': 'radar_2', '3_radar': 'radar_3', '4_radar': 'radar_4', '5_radar': 'radar_5', '6_radar': 'radar_6', '7_radar': 'radar_7'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the columns to float\n",
    "df_building[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']] = df_building[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']].astype(float)\n",
    "df_bikelane[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']] = df_bikelane[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data as CSV for EDA purposes\n",
    "# df_building.to_csv('df_building_weather.csv', index=False)\n",
    "# df_bikelane.to_csv('df_bikelane_weather.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "In this step, we develop a method to label the data points as either 'detection' or 'no_detection.' \n",
    "\n",
    "Our exploratory data analysis revealed that the electromagnetic field data is best suited for our objectives. Therefore, we focus on the x, y, and z data from this dataset.\n",
    "\n",
    "In our case, we chose KMeans as our clustering method and used the magnetic sensor data from the x, y, and z axes as features. This is done after normalizing the data using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dataframe for the features we wish to cluster on\n",
    "building_mag = df_building[[\"x\",\"y\",\"z\"]]\n",
    "bikelane_mag = df_bikelane[[\"x\",\"y\",\"z\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "building_mag_norm = scaler.fit_transform(building_mag)\n",
    "bikelane_mag_norm = scaler.fit_transform(bikelane_mag)\n",
    "# Clustering the magnetic field data with 2 clusters using kmeans\n",
    "building_kmeans = KMeans(n_clusters=2, random_state=0).fit(building_mag_norm)\n",
    "bikelane_kmeans = KMeans(n_clusters=2, random_state=0).fit(bikelane_mag_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding cluster labels to the mag dataframe\n",
    "building_mag = building_mag.copy() #dealing with an error\n",
    "bikelane_mag = bikelane_mag.copy() #dealing with an error\n",
    "building_mag['mag_cluster'] = building_kmeans.labels_\n",
    "bikelane_mag['mag_cluster'] = bikelane_kmeans.labels_\n",
    "df_building = df_building.copy() #dealing with an error\n",
    "df_bikelane = df_bikelane.copy() #dealing with an error\n",
    "df_building['mag_cluster'] = building_mag['mag_cluster']\n",
    "df_bikelane['mag_cluster'] = bikelane_mag['mag_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the cluster labels to 'detection' and 'no_detection'\n",
    "df_building['mag_cluster'].replace({0: 'no_detection', 1: 'detection'}, inplace=True)\n",
    "df_bikelane['mag_cluster'].replace({0: 'no_detection', 1: 'detection'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing an error with the mag_cluster column type\n",
    "df_building['mag_cluster'] = df_building['mag_cluster'].astype(str)\n",
    "df_building['mag_cluster'].replace('nan', None, inplace=True)  # Replace 'nan' string with actual None\n",
    "df_bikelane['mag_cluster'] = df_bikelane['mag_cluster'].astype(str)\n",
    "df_bikelane['mag_cluster'].replace('nan', None, inplace=True)  # Replace 'nan' string with actual None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hopsworks Feature Storage\n",
    "\n",
    "Now we would like to connect to the Hopsworks Feature Store so we can access and create feature groups.\n",
    "\n",
    "In creating feature groups we take all the relevant coulmns and store it in hopworks, so that we later can acces and interperet for further use.\n",
    "\n",
    "We also specify a 'primary_key' that is used for relating diferent dimention tables to each other, in our case this is the unique ID that we made in the preprocessing step. \n",
    "\n",
    "The 'time' column is used as the event time key.\n",
    "\n",
    "we also we put `online_enabled` to `True` to make the feature group online for acces with an API when we make feature views.\n",
    "\n",
    "And finally we give descriptions to each coulmn with information given by the *README.txt* in *data*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy your Api Key (first register/login): https://c.app.hopsworks.ai/account/api/generated\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Multiple projects found. \n",
      "\n",
      "\t (1) annikaij\n",
      "\t (2) miknie20\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/549019\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Connceting to the Hopsworks project\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create a feature group for the parking spot close to the building\n",
    "api_hist_building_fg = fs.get_or_create_feature_group(\n",
    "    name=\"api_building_detection_features\",\n",
    "    version=1,\n",
    "    description=\"Data from API for parking detection on the parkingspot close to the building\",\n",
    "    primary_key=['id'],\n",
    "    event_time='time',\n",
    "    online_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/549019/fs/544841/fg/794956\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42921f8fdb6447a896f6e722929688d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/7155 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: api_building_detection_features_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/549019/jobs/named/api_building_detection_features_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7f936758ae90>, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the magnetic field features into the feature group\n",
    "api_hist_building_fg.insert(df_building, write_options={\"wait_for_job\" : False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Making descriptions for the features\n",
    "feature_descriptions = [\n",
    "    {\"name\": \"time\", \"description\": \"Timepoint of the datapoint\"},\n",
    "    {\"name\": \"battery\", \"description\": \"Battery level of the sensor\"},\n",
    "    {\"name\": \"temperature\", \"description\": \"Temperature recorded by the sensor\"},\n",
    "    {\"name\": \"x\", \"description\": \"Magnetic field reading in the x direction\"},\n",
    "    {\"name\": \"y\", \"description\": \"Magnetic field reading in the y direction\"},\n",
    "    {\"name\": \"z\", \"description\": \"Magnetic field reading in the z direction\"},\n",
    "    {\"name\": \"radar_0\", \"description\": \"Radar reading from sensor radar sensor 0\"},\n",
    "    {\"name\": \"radar_1\", \"description\": \"Radar reading from sensor radar sensor 1\"},\n",
    "    {\"name\": \"radar_2\", \"description\": \"Radar reading from sensor radar sensor 2\"},\n",
    "    {\"name\": \"radar_3\", \"description\": \"Radar reading from sensor radar sensor 3\"},\n",
    "    {\"name\": \"radar_4\", \"description\": \"Radar reading from sensor radar sensor 4\"},\n",
    "    {\"name\": \"radar_5\", \"description\": \"Radar reading from sensor radar sensor 5\"},\n",
    "    {\"name\": \"radar_6\", \"description\": \"Radar reading from sensor radar sensor 6\"},\n",
    "    {\"name\": \"radar_7\", \"description\": \"Radar reading from sensor radar sensor 7\"},\n",
    "    {\"name\": \"package_type\", \"description\": \"Heartbeat indicates no significant change since last reading or change package type means that x, y or z has changed significantly +-30\"},\n",
    "    {\"name\": \"f_cnt\", \"description\": \"number of packages transmitted since last network registration\"},\n",
    "    {\"name\": \"dr\", \"description\": \"data rate parameter in LoRaWAN. It ranges between 1 and 5 where 1 is the slowest transmission data rate and 5 is the highest. This datarate is scaled by the network server depending on the signal quality of the past packages send\"},\n",
    "    {\"name\": \"snr\", \"description\": \"signal to noise ratio – the higher value, the better the signal quality\"},\n",
    "    {\"name\": \"rssi\", \"description\": \"signal strength – the higher value, the better the signal quality\"},\n",
    "    {\"name\": \"psensor\", \"description\": \"sensor identifier (ex. EL1, EL2, EL3)\"},\n",
    "    {\"name\": \"hw_fw_version\", \"description\": \"hardware and firmware version of the sensor\"},\n",
    "    {\"name\": \"id\", \"description\": \"unique identifier for each datapoint made uuid4\"}\n",
    "]\n",
    "\n",
    "for desc in feature_descriptions: \n",
    "    api_hist_building_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create a feature group for the parking spot close to the bike lane\n",
    "api_hist_bikelane_fg = fs.get_or_create_feature_group(\n",
    "    name=\"api_bikelane_detection_features\",\n",
    "    version=1,\n",
    "    description=\"Data from API for parking detection on the parkingspot close to the bikelane\",\n",
    "    primary_key=['id'],\n",
    "    event_time='time',\n",
    "    online_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/549019/fs/544841/fg/793932\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b48366c671b431bb35010db63f2d58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/7047 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: api_bikelane_detection_features_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/549019/jobs/named/api_bikelane_detection_features_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7f936755ff50>, None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the magnetic field features into the feature group\n",
    "api_hist_bikelane_fg.insert(df_bikelane, write_options={\"wait_for_job\" : False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Making descriptions for the features\n",
    "feature_descriptions = [\n",
    "    {\"name\": \"time\", \"description\": \"Timepoint of the datapoint\"},\n",
    "    {\"name\": \"battery\", \"description\": \"Battery level of the sensor\"},\n",
    "    {\"name\": \"temperature\", \"description\": \"Temperature recorded by the sensor\"},\n",
    "    {\"name\": \"x\", \"description\": \"Magnetic field reading in the x direction\"},\n",
    "    {\"name\": \"y\", \"description\": \"Magnetic field reading in the y direction\"},\n",
    "    {\"name\": \"z\", \"description\": \"Magnetic field reading in the z direction\"},\n",
    "    {\"name\": \"radar_0\", \"description\": \"Radar reading from sensor radar sensor 0\"},\n",
    "    {\"name\": \"radar_1\", \"description\": \"Radar reading from sensor radar sensor 1\"},\n",
    "    {\"name\": \"radar_2\", \"description\": \"Radar reading from sensor radar sensor 2\"},\n",
    "    {\"name\": \"radar_3\", \"description\": \"Radar reading from sensor radar sensor 3\"},\n",
    "    {\"name\": \"radar_4\", \"description\": \"Radar reading from sensor radar sensor 4\"},\n",
    "    {\"name\": \"radar_5\", \"description\": \"Radar reading from sensor radar sensor 5\"},\n",
    "    {\"name\": \"radar_6\", \"description\": \"Radar reading from sensor radar sensor 6\"},\n",
    "    {\"name\": \"radar_7\", \"description\": \"Radar reading from sensor radar sensor 7\"},\n",
    "    {\"name\": \"package_type\", \"description\": \"Heartbeat indicates no significant change since last reading or change package type means that x, y or z has changed significantly +-30\"},\n",
    "    {\"name\": \"f_cnt\", \"description\": \"number of packages transmitted since last network registration\"},\n",
    "    {\"name\": \"dr\", \"description\": \"data rate parameter in LoRaWAN. It ranges between 1 and 5 where 1 is the slowest transmission data rate and 5 is the highest. This datarate is scaled by the network server depending on the signal quality of the past packages send\"},\n",
    "    {\"name\": \"snr\", \"description\": \"signal to noise ratio – the higher value, the better the signal quality\"},\n",
    "    {\"name\": \"rssi\", \"description\": \"signal strength – the higher value, the better the signal quality\"},\n",
    "    {\"name\": \"psensor\", \"description\": \"sensor identifier (ex. EL1, EL2, EL3)\"},\n",
    "    {\"name\": \"hw_fw_version\", \"description\": \"hardware and firmware version of the sensor\"},\n",
    "    {\"name\": \"id\", \"description\": \"unique identifier for each datapoint made uuid4\"}\n",
    "]\n",
    "\n",
    "for desc in feature_descriptions: \n",
    "    api_hist_bikelane_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next up:** 2: Latest API data\n",
    "Go to the 2_latest_api_feature_pipeline.ipynb notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
