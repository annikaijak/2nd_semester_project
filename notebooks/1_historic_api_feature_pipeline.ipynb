{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill and Feature Engineering Notebook\n",
    "This notebook consists of 5 parts:\n",
    "1. Importing libaries and loading packages\n",
    "2. Data Loading\n",
    "3. Data Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Hopsworks Feature Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, our decision-making process is informed by insights gained from the exploratory data analysis (EDA) we conducted. This analysis helped us identify the most relevant information for our methods and strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libaries and loading packages\n",
    "In this section, we import necessary libraries and define key functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Package for hopsworks integration\n",
    "# !pip install -U hopsworks --quiet\n",
    "\n",
    "# Import standard Python libraries\n",
    "import pandas as pd \n",
    "import hopsworks \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import machine learning tools\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.cluster import KMeans  \n",
    "from sklearn.metrics import silhouette_score  \n",
    "\n",
    "# Import other useful libraries\n",
    "import uuid  # Unique identifier generation\n",
    "import requests  # For making API requests\n",
    "import json  \n",
    "import io \n",
    "import os\n",
    "import base64 \n",
    "from datetime import datetime, timedelta  # Date/time handling and manipulation\n",
    "import pytz  # Timezone conversions and support\n",
    "\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# Environment variable management\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Sensor Data\n",
    "We load historic data to be used in the notebook.\n",
    "\n",
    "- The data is gathered trough an API given by the company, and the API data results are stored in the *bikelane_historic_data.csv* and *building_historic_data.csv*. The data loacated in the *data* folder is old data that the EDA and end-to-end-pipeline is made from. Here you can also find more information about the data in the readme.txt file.\n",
    "\n",
    "- The data derivied from the API is from two parking spots, one of the spots are close to a building and the other close to a bikelane, and will be refered to with this as the identifyer.\n",
    "\n",
    "- In this section we will be pinging the API and saving historic data from march and april in two csv-files, containing data from the parking spot close to the building and the one close to the bikelane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-24 12:35:02.881736\n"
     ]
    }
   ],
   "source": [
    "# getting the time for now, tomorrow and yesterday\n",
    "now = datetime.now()  # Get current time \n",
    "today = now \n",
    "yesterday = today - timedelta(days=1)\n",
    "tomorrow = today + timedelta(days=1)\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format 'today', 'tomorrow', and 'yesterday' as \"YYYY-MM-DD\"\n",
    "formatted_today = today.strftime('%Y-%m-%d %H:%M:%S')\n",
    "formatted_tomorrow = tomorrow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "formatted_yesterday = yesterday.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining API information\n",
    "dev_eui_building = \"0080E115003BEA91\"\n",
    "dev_eui_bikelane = \"0080E115003E3597\"\n",
    "url = \"https://data.sensade.com\"\n",
    "\n",
    "basic_auth = base64.b64encode(f\"{os.getenv('API_USERNAME')}:{os.getenv('API_PASSWORD')}\".encode())\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Basic {basic_auth.decode(\"utf-8\")}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to ping the API and get data in a given time interval\n",
    "def API_call(dev_eui, from_date, to_date):\n",
    "    payload = json.dumps({\n",
    "    \"dev_eui\": dev_eui,\n",
    "    \"from\": from_date,\n",
    "    \"to\": to_date\n",
    "})\n",
    "\n",
    "    API_response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "\n",
    "    if API_response.status_code != 200:\n",
    "        exit(13)\n",
    "\n",
    "    csv_data = API_response.text\n",
    "    df = pd.read_csv(io.StringIO(csv_data))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section below is commented out because the files are already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# march_building = API_call(dev_eui_building, \"2024-03-01\", \"2024-04-01\")\n",
    "# march_bikelane = API_call(dev_eui_bikelane, \"2024-03-01\", \"2024-04-01\")\n",
    "# april_building = API_call(dev_eui_building, \"2024-04-01\", \"2024-05-01\")\n",
    "# april_bikelane = API_call(dev_eui_bikelane, \"2024-04-01\", \"2024-05-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building_historic_df = pd.concat([march_building, april_building], ignore_index=True)\n",
    "# bikelane_historic_df = pd.concat([march_bikelane, april_bikelane], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data as CSV\n",
    "# building_historic_df.to_csv('building_historic_df.csv', index=False)\n",
    "# bikelane_historic_df.to_csv('bikelane_historic_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data saved in the directory\n",
    "building_historic_df = pd.read_csv('building_historic_df.csv')\n",
    "bikelane_historic_df = pd.read_csv('bikelane_historic_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that tries to parse the datetime with microseconds first, and if it fails, parses it without microseconds\n",
    "def parse_datetime(dt_str):\n",
    "    try:\n",
    "        return datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        return datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function on the dataframes\n",
    "building_historic_df = building_historic_df.copy()\n",
    "building_historic_df['time'] = building_historic_df['time'].apply(parse_datetime)\n",
    "bikelane_historic_df = bikelane_historic_df.copy()\n",
    "bikelane_historic_df['time'] = bikelane_historic_df['time'].apply(parse_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column for the time in the format of \"YYYY-MM-DD HH\" to merge with weather data\n",
    "bikelane_historic_df['time_hour'] = bikelane_historic_df['time'].dt.strftime('%Y-%m-%d %H')\n",
    "building_historic_df['time_hour'] = building_historic_df['time'].dt.strftime('%Y-%m-%d %H')\n",
    "\n",
    "# Converting the time_hour column to datetime\n",
    "bikelane_historic_df['time_hour'] = pd.to_datetime(bikelane_historic_df['time_hour'])\n",
    "building_historic_df['time_hour'] = pd.to_datetime(building_historic_df['time_hour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "weather_params = {\n",
    "\t\"latitude\": 57.01,\n",
    "\t\"longitude\": 9.99,\n",
    "\t\"start_date\": \"2024-03-01\",\n",
    "\t\"end_date\": \"2024-04-30\",\n",
    "\t\"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"surface_pressure\", \"cloud_cover\", \"et0_fao_evapotranspiration\", \"wind_speed_10m\", \"soil_temperature_0_to_7cm\", \"soil_moisture_0_to_7cm\"]\n",
    "}\n",
    "responses = openmeteo.weather_api(weather_url, params=weather_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 56.977149963378906°N 10.0632905960083°E\n",
      "Elevation 23.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>relative_humidity_2m</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>surface_pressure</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>et0_fao_evapotranspiration</th>\n",
       "      <th>wind_speed_10m</th>\n",
       "      <th>soil_temperature_0_to_7cm</th>\n",
       "      <th>soil_moisture_0_to_7cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-01 00:00:00+00:00</td>\n",
       "      <td>6.2305</td>\n",
       "      <td>87.634949</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1001.978699</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.006267</td>\n",
       "      <td>16.904673</td>\n",
       "      <td>5.3805</td>\n",
       "      <td>0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-01 01:00:00+00:00</td>\n",
       "      <td>5.7305</td>\n",
       "      <td>88.205788</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1002.073425</td>\n",
       "      <td>80.400009</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>16.299694</td>\n",
       "      <td>5.1305</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-01 02:00:00+00:00</td>\n",
       "      <td>5.2305</td>\n",
       "      <td>89.097878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001.470093</td>\n",
       "      <td>60.600002</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>16.873980</td>\n",
       "      <td>4.9305</td>\n",
       "      <td>0.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-01 03:00:00+00:00</td>\n",
       "      <td>4.7805</td>\n",
       "      <td>90.966789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.767395</td>\n",
       "      <td>39.600002</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>16.981165</td>\n",
       "      <td>4.7305</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-01 04:00:00+00:00</td>\n",
       "      <td>3.6805</td>\n",
       "      <td>94.174324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.855896</td>\n",
       "      <td>53.400002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.076300</td>\n",
       "      <td>4.2305</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  temperature_2m  relative_humidity_2m  \\\n",
       "0 2024-03-01 00:00:00+00:00          6.2305             87.634949   \n",
       "1 2024-03-01 01:00:00+00:00          5.7305             88.205788   \n",
       "2 2024-03-01 02:00:00+00:00          5.2305             89.097878   \n",
       "3 2024-03-01 03:00:00+00:00          4.7805             90.966789   \n",
       "4 2024-03-01 04:00:00+00:00          3.6805             94.174324   \n",
       "\n",
       "   precipitation  surface_pressure  cloud_cover  et0_fao_evapotranspiration  \\\n",
       "0            0.1       1001.978699   100.000000                    0.006267   \n",
       "1            0.1       1002.073425    80.400009                    0.004743   \n",
       "2            0.0       1001.470093    60.600002                    0.003818   \n",
       "3            0.0       1000.767395    39.600002                    0.001230   \n",
       "4            0.0       1000.855896    53.400002                    0.000000   \n",
       "\n",
       "   wind_speed_10m  soil_temperature_0_to_7cm  soil_moisture_0_to_7cm  \n",
       "0       16.904673                     5.3805                   0.296  \n",
       "1       16.299694                     5.1305                   0.292  \n",
       "2       16.873980                     4.9305                   0.289  \n",
       "3       16.981165                     4.7305                   0.286  \n",
       "4       17.076300                     4.2305                   0.284  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}°N {response.Longitude()}°E\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_surface_pressure = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_cloud_cover = hourly.Variables(4).ValuesAsNumpy()\n",
    "hourly_et0_fao_evapotranspiration = hourly.Variables(5).ValuesAsNumpy()\n",
    "hourly_wind_speed_10m = hourly.Variables(6).ValuesAsNumpy()\n",
    "hourly_soil_temperature_0_to_7cm = hourly.Variables(7).ValuesAsNumpy()\n",
    "hourly_soil_moisture_0_to_7cm = hourly.Variables(8).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
    "hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "hourly_data[\"surface_pressure\"] = hourly_surface_pressure\n",
    "hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "hourly_data[\"et0_fao_evapotranspiration\"] = hourly_et0_fao_evapotranspiration\n",
    "hourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
    "hourly_data[\"soil_temperature_0_to_7cm\"] = hourly_soil_temperature_0_to_7cm\n",
    "hourly_data[\"soil_moisture_0_to_7cm\"] = hourly_soil_moisture_0_to_7cm\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "hourly_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the timezone from the date column\n",
    "hourly_dataframe['date'] = hourly_dataframe['date'].dt.tz_localize(None)\n",
    "#Convert to datetime object\n",
    "hourly_dataframe['date'] = pd.to_datetime(hourly_dataframe['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merging Sensor and Weather Data\n",
    "Now before just dumping the data into a feature store we do a little preprocessing to enhance the use of our datasets.\n",
    "\n",
    "This preprocessing consists of:\n",
    "- Making unique identifyers for each datapoint\n",
    "- Combining the three datasets into one \n",
    "- Making clusters used for labeling, which is nessesary when we want to train our models later\n",
    "- Converting the data column to pandas datetime\n",
    "- minor adjustments for the naming of radar columns to fix some hopsworks problem where the name of the columns cannot start with a number, and making the relevant columns into float format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the weather data with the building sensor data\n",
    "building_historic_df = pd.merge(building_historic_df, hourly_dataframe, left_on='time_hour', right_on='date', how='left')\n",
    "# Merging the weather data with the bikelane sensor data\n",
    "bikelane_historic_df = pd.merge(bikelane_historic_df, hourly_dataframe, left_on='time_hour', right_on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing date column\n",
    "building_historic_df = building_historic_df.drop(columns=['date'])\n",
    "bikelane_historic_df = bikelane_historic_df.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique identifier for each row in the datasets\n",
    "def create_id(df, dataset_name):\n",
    "    # Assign the sensor prefix based on the dataset name\n",
    "    if dataset_name == 'building_historic_df':\n",
    "        df['psensor'] = \"BUILDING\"\n",
    "    elif dataset_name == 'bikelane_historic_df':\n",
    "        df['psensor'] = \"BIKELANE\"\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name provided\")\n",
    "\n",
    "    # Create a new column 'id' with a unique identifier for each row\n",
    "    df['id'] = df['time'].astype(str) + '_' + df['psensor']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the ID creator function to the datasets\n",
    "df_bikelane = create_id(bikelane_historic_df, 'bikelane_historic_df')\n",
    "df_building = create_id(building_historic_df, 'building_historic_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the radar columns to start with radar to deal with hopsworks problem\n",
    "df_building = df_building.rename(columns={'0_radar': 'radar_0', '1_radar': 'radar_1', '2_radar': 'radar_2', '3_radar': 'radar_3', '4_radar': 'radar_4', '5_radar': 'radar_5', '6_radar': 'radar_6', '7_radar': 'radar_7'})\n",
    "df_bikelane = df_bikelane.rename(columns={'0_radar': 'radar_0', '1_radar': 'radar_1', '2_radar': 'radar_2', '3_radar': 'radar_3', '4_radar': 'radar_4', '5_radar': 'radar_5', '6_radar': 'radar_6', '7_radar': 'radar_7'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the columns to float\n",
    "df_building[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']] = df_building[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']].astype(float)\n",
    "df_bikelane[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']] = df_bikelane[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating full dataframe with backfilled battery and radar columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_full_df = df_building.copy()\n",
    "bikelane_full_df = df_bikelane.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backfill missing values in radar columns and battery column with the previous value\n",
    "building_full_df['radar_0'] = building_full_df['radar_0'].bfill()\n",
    "building_full_df['radar_1'] = building_full_df['radar_1'].bfill()\n",
    "building_full_df['radar_2'] = building_full_df['radar_2'].bfill()\n",
    "building_full_df['radar_3'] = building_full_df['radar_3'].bfill()\n",
    "building_full_df['radar_4'] = building_full_df['radar_4'].bfill()\n",
    "building_full_df['radar_5'] = building_full_df['radar_5'].bfill()\n",
    "building_full_df['radar_6'] = building_full_df['radar_6'].bfill()\n",
    "building_full_df['radar_7'] = building_full_df['radar_7'].bfill()\n",
    "building_full_df['battery'] = building_full_df['battery'].bfill()\n",
    "bikelane_full_df['radar_0'] = bikelane_full_df['radar_0'].bfill()\n",
    "bikelane_full_df['radar_1'] = bikelane_full_df['radar_1'].bfill()\n",
    "bikelane_full_df['radar_2'] = bikelane_full_df['radar_2'].bfill()\n",
    "bikelane_full_df['radar_3'] = bikelane_full_df['radar_3'].bfill()\n",
    "bikelane_full_df['radar_4'] = bikelane_full_df['radar_4'].bfill()\n",
    "bikelane_full_df['radar_5'] = bikelane_full_df['radar_5'].bfill()\n",
    "bikelane_full_df['radar_6'] = bikelane_full_df['radar_6'].bfill()\n",
    "bikelane_full_df['radar_7'] = bikelane_full_df['radar_7'].bfill()\n",
    "bikelane_full_df['battery'] = bikelane_full_df['battery'].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7667, 32)\n",
      "(7595, 32)\n"
     ]
    }
   ],
   "source": [
    "print(building_full_df.shape)\n",
    "print(bikelane_full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping observations with missing values in the radar columns\n",
    "building_full_df = building_full_df.dropna(subset=['radar_0'])\n",
    "bikelane_full_df = bikelane_full_df.dropna(subset=['radar_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataframe with radar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_radar_df = df_building[['time', 'battery', 'temperature', 'radar_0', 'radar_1',\n",
    "       'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7',\n",
    "       'package_type', 'f_cnt', 'dr', 'snr', 'rssi', 'hw_fw_version',\n",
    "       'time_hour', 'temperature_2m', 'relative_humidity_2m', 'precipitation',\n",
    "       'surface_pressure', 'cloud_cover', 'et0_fao_evapotranspiration',\n",
    "       'wind_speed_10m', 'soil_temperature_0_to_7cm', 'soil_moisture_0_to_7cm',\n",
    "       'psensor', 'id']]\n",
    "bikelane_radar_df = df_bikelane[['time', 'battery', 'temperature', 'radar_0', 'radar_1',\n",
    "       'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7',\n",
    "       'package_type', 'f_cnt', 'dr', 'snr', 'rssi', 'hw_fw_version',\n",
    "       'time_hour', 'temperature_2m', 'relative_humidity_2m', 'precipitation',\n",
    "       'surface_pressure', 'cloud_cover', 'et0_fao_evapotranspiration',\n",
    "       'wind_speed_10m', 'soil_temperature_0_to_7cm', 'soil_moisture_0_to_7cm',\n",
    "       'psensor', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_radar_df = building_radar_df.dropna(subset=['battery'])\n",
    "bikelane_radar_df = bikelane_radar_df.dropna(subset=['battery'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2685, 29)\n",
      "(2628, 29)\n"
     ]
    }
   ],
   "source": [
    "print(building_radar_df.shape)\n",
    "print(bikelane_radar_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnetic Field Clustering\n",
    "In this step, we develop a method to label the data points as either 'detection' or 'no_detection.' \n",
    "\n",
    "Our exploratory data analysis revealed that the electromagnetic field data is best suited for our objectives. Therefore, we focus on the x, y, and z data from this dataset.\n",
    "\n",
    "In our case, we chose KMeans as our clustering method and used the magnetic sensor data from the x, y, and z axes as features. This is done after normalizing the data using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dataframe for the features we wish to cluster on\n",
    "building_mag = building_full_df[[\"x\",\"y\",\"z\"]]\n",
    "bikelane_mag = bikelane_full_df[[\"x\",\"y\",\"z\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "building_mag_norm = scaler.fit_transform(building_mag)\n",
    "bikelane_mag_norm = scaler.fit_transform(bikelane_mag)\n",
    "# Clustering the magnetic field data with 2 clusters using kmeans\n",
    "building_kmeans = KMeans(n_clusters=2, random_state=0).fit(building_mag_norm)\n",
    "bikelane_kmeans = KMeans(n_clusters=2, random_state=0).fit(bikelane_mag_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding cluster labels to the mag dataframe\n",
    "building_mag = building_mag.copy() #dealing with an error\n",
    "bikelane_mag = bikelane_mag.copy() #dealing with an error\n",
    "building_mag['mag_cluster'] = building_kmeans.labels_\n",
    "bikelane_mag['mag_cluster'] = bikelane_kmeans.labels_\n",
    "building_full_df = building_full_df.copy() #dealing with an error\n",
    "bikelane_full_df = bikelane_full_df.copy() #dealing with an error\n",
    "building_full_df['mag_cluster'] = building_mag['mag_cluster']\n",
    "bikelane_full_df['mag_cluster'] = bikelane_mag['mag_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the cluster labels to 'detection' and 'no_detection'\n",
    "building_full_df['mag_cluster'].replace({0: 'no_detection', 1: 'detection'}, inplace=True)\n",
    "bikelane_full_df['mag_cluster'].replace({0: 'no_detection', 1: 'detection'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing an error with the mag_cluster column type\n",
    "building_full_df['mag_cluster'] = building_full_df['mag_cluster'].astype(str)\n",
    "building_full_df['mag_cluster'].replace('nan', None, inplace=True)  # Replace 'nan' string with actual None\n",
    "bikelane_full_df['mag_cluster'] = bikelane_full_df['mag_cluster'].astype(str)\n",
    "bikelane_full_df['mag_cluster'].replace('nan', None, inplace=True)  # Replace 'nan' string with actual None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mag_cluster\n",
       "no_detection    7077\n",
       "detection        588\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "building_full_df['mag_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mag_cluster\n",
       "no_detection    5006\n",
       "detection       2589\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikelane_full_df['mag_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data as CSV for EDA purposes\n",
    "building_full_df.to_csv('building_full_df.csv', index=False)\n",
    "bikelane_full_df.to_csv('bikelane_full_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with radar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dataframe for the features we wish to cluster on\n",
    "building_radar = building_radar_df[['radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7']]\n",
    "bikelane_radar = bikelane_radar_df[['radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "building_radar_norm = scaler.fit_transform(building_radar)\n",
    "bikelane_radar_norm = scaler.fit_transform(bikelane_radar)\n",
    "# Clustering the magnetic field data with 2 clusters using kmeans\n",
    "building_kmeans = KMeans(n_clusters=2, random_state=0).fit(building_radar_norm)\n",
    "bikelane_kmeans = KMeans(n_clusters=2, random_state=0).fit(bikelane_radar_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding cluster labels to the radar dataframe\n",
    "building_radar = building_radar.copy() #dealing with an error\n",
    "bikelane_radar = bikelane_radar.copy() #dealing with an error\n",
    "building_radar['radar_cluster'] = building_kmeans.labels_\n",
    "bikelane_radar['radar_cluster'] = bikelane_kmeans.labels_\n",
    "building_radar_df = building_radar_df.copy() #dealing with an error\n",
    "bikelane_radar_df = bikelane_radar_df.copy() #dealing with an error\n",
    "building_radar_df['radar_cluster'] = building_radar['radar_cluster']\n",
    "bikelane_radar_df['radar_cluster'] = bikelane_radar['radar_cluster']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the cluster labels to 'detection' and 'no_detection'\n",
    "building_radar_df['radar_cluster'].replace({0: 'no_detection', 1: 'detection'}, inplace=True)\n",
    "bikelane_radar_df['radar_cluster'].replace({0: 'no_detection', 1: 'detection'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "radar_cluster\n",
       "no_detection    2248\n",
       "detection        437\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "building_radar_df['radar_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "radar_cluster\n",
       "no_detection    2204\n",
       "detection        424\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikelane_radar_df['radar_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the data as CSV for EDA purposes\n",
    "building_radar_df.to_csv('building_radar_df.csv', index=False)\n",
    "bikelane_radar_df.to_csv('bikelane_radar_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining building and bikelane dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_full_df = pd.concat([building_full_df, bikelane_full_df], axis=0)\n",
    "combined_radar_df = pd.concat([building_radar_df, bikelane_radar_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'battery', 'temperature', 'radar_0', 'radar_1', 'radar_2',\n",
       "       'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'package_type',\n",
       "       'f_cnt', 'dr', 'snr', 'rssi', 'hw_fw_version', 'time_hour',\n",
       "       'temperature_2m', 'relative_humidity_2m', 'precipitation',\n",
       "       'surface_pressure', 'cloud_cover', 'et0_fao_evapotranspiration',\n",
       "       'wind_speed_10m', 'soil_temperature_0_to_7cm', 'soil_moisture_0_to_7cm',\n",
       "       'psensor', 'id', 'radar_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_radar_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_cluster = combined_radar_df[['id', 'radar_cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_full_df = pd.merge(combined_full_df, radar_cluster, left_on='id', right_on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_full_df['radar_cluster'] = combined_full_df['radar_cluster'].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_full_df.to_csv('combined_full_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hopsworks Feature Storage\n",
    "\n",
    "Now we would like to connect to the Hopsworks Feature Store so we can access and create feature groups.\n",
    "\n",
    "In creating feature groups we take all the relevant coulmns and store it in hopworks, so that we later can acces and interperet for further use.\n",
    "\n",
    "We also specify a 'primary_key' that is used for relating diferent dimention tables to each other, in our case this is the unique ID that we made in the preprocessing step. \n",
    "\n",
    "The 'time' column is used as the event time key.\n",
    "\n",
    "we also we put `online_enabled` to `True` to make the feature group online for acces with an API when we make feature views.\n",
    "\n",
    "And finally we give descriptions to each coulmn with information given by the *README.txt* in *data*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/549019\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Connceting to the Hopsworks project\n",
    "project = hopsworks.login(project=\"annikaij\")\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create a feature group for the parking spot close to the building\n",
    "hist_combined_full_fg = fs.get_or_create_feature_group(\n",
    "    name=\"hist_combined_full_fg\",\n",
    "    version=1,\n",
    "    description=\"Data from Sensor and Weather API for both Parking Spots\",\n",
    "    primary_key=['id'],\n",
    "    event_time='time',\n",
    "    online_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/549019/fs/544841/fg/844145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ce53f1472640beb44c9f93ac76b7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/15260 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: hist_combined_full_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/549019/jobs/named/hist_combined_full_fg_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7fa302b77c50>, None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the magnetic field features into the feature group\n",
    "hist_combined_full_fg.insert(combined_full_df, write_options={\"wait_for_job\" : False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Making descriptions for the features\n",
    "# feature_descriptions = [\n",
    "#     {\"name\": \"time\", \"description\": \"Timepoint of the datapoint\"},\n",
    "#     {\"name\": \"battery\", \"description\": \"Battery level of the sensor\"},\n",
    "#     {\"name\": \"temperature\", \"description\": \"Temperature recorded by the sensor\"},\n",
    "#     {\"name\": \"x\", \"description\": \"Magnetic field reading in the x direction\"},\n",
    "#     {\"name\": \"y\", \"description\": \"Magnetic field reading in the y direction\"},\n",
    "#     {\"name\": \"z\", \"description\": \"Magnetic field reading in the z direction\"},\n",
    "#     {\"name\": \"radar_0\", \"description\": \"Radar reading from sensor radar sensor 0\"},\n",
    "#     {\"name\": \"radar_1\", \"description\": \"Radar reading from sensor radar sensor 1\"},\n",
    "#     {\"name\": \"radar_2\", \"description\": \"Radar reading from sensor radar sensor 2\"},\n",
    "#     {\"name\": \"radar_3\", \"description\": \"Radar reading from sensor radar sensor 3\"},\n",
    "#     {\"name\": \"radar_4\", \"description\": \"Radar reading from sensor radar sensor 4\"},\n",
    "#     {\"name\": \"radar_5\", \"description\": \"Radar reading from sensor radar sensor 5\"},\n",
    "#     {\"name\": \"radar_6\", \"description\": \"Radar reading from sensor radar sensor 6\"},\n",
    "#     {\"name\": \"radar_7\", \"description\": \"Radar reading from sensor radar sensor 7\"},\n",
    "#     {\"name\": \"package_type\", \"description\": \"Heartbeat indicates no significant change since last reading or change package type means that x, y or z has changed significantly +-30\"},\n",
    "#     {\"name\": \"f_cnt\", \"description\": \"number of packages transmitted since last network registration\"},\n",
    "#     {\"name\": \"dr\", \"description\": \"data rate parameter in LoRaWAN. It ranges between 1 and 5 where 1 is the slowest transmission data rate and 5 is the highest. This datarate is scaled by the network server depending on the signal quality of the past packages send\"},\n",
    "#     {\"name\": \"snr\", \"description\": \"signal to noise ratio – the higher value, the better the signal quality\"},\n",
    "#     {\"name\": \"rssi\", \"description\": \"signal strength – the higher value, the better the signal quality\"},\n",
    "#     {\"name\": \"psensor\", \"description\": \"sensor identifier (ex. EL1, EL2, EL3)\"},\n",
    "#     {\"name\": \"hw_fw_version\", \"description\": \"hardware and firmware version of the sensor\"},\n",
    "#     {\"name\": \"id\", \"description\": \"unique identifier for each datapoint made uuid4\"}\n",
    "# ]\n",
    "# \n",
    "# for desc in feature_descriptions: \n",
    "#     api_hist_building_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create a feature group for the parking spot close to the bike lane\n",
    "hist_combined_radar_fg = fs.get_or_create_feature_group(\n",
    "    name=\"hist_combined_radar_fg\",\n",
    "    version=1,\n",
    "    description=\"Data from Sensor and Weather API for both Parking Spots but only with Non-Backfilled Radar Data\",\n",
    "    primary_key=['id'],\n",
    "    event_time='time',\n",
    "    online_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/549019/fs/544841/fg/845145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee4a66bf8a94e67bea063ebe7dcba7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/5313 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: hist_combined_radar_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/549019/jobs/named/hist_combined_radar_fg_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7fa30486f750>, None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the magnetic field features into the feature group\n",
    "hist_combined_radar_fg.insert(combined_radar_df, write_options={\"wait_for_job\" : False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Making descriptions for the features\n",
    "# feature_descriptions = [\n",
    "#     {\"name\": \"time\", \"description\": \"Timepoint of the datapoint\"},\n",
    "#     {\"name\": \"battery\", \"description\": \"Battery level of the sensor\"},\n",
    "#     {\"name\": \"temperature\", \"description\": \"Temperature recorded by the sensor\"},\n",
    "#     {\"name\": \"x\", \"description\": \"Magnetic field reading in the x direction\"},\n",
    "#     {\"name\": \"y\", \"description\": \"Magnetic field reading in the y direction\"},\n",
    "#     {\"name\": \"z\", \"description\": \"Magnetic field reading in the z direction\"},\n",
    "#     {\"name\": \"radar_0\", \"description\": \"Radar reading from sensor radar sensor 0\"},\n",
    "#     {\"name\": \"radar_1\", \"description\": \"Radar reading from sensor radar sensor 1\"},\n",
    "#     {\"name\": \"radar_2\", \"description\": \"Radar reading from sensor radar sensor 2\"},\n",
    "#     {\"name\": \"radar_3\", \"description\": \"Radar reading from sensor radar sensor 3\"},\n",
    "#     {\"name\": \"radar_4\", \"description\": \"Radar reading from sensor radar sensor 4\"},\n",
    "#     {\"name\": \"radar_5\", \"description\": \"Radar reading from sensor radar sensor 5\"},\n",
    "#     {\"name\": \"radar_6\", \"description\": \"Radar reading from sensor radar sensor 6\"},\n",
    "#     {\"name\": \"radar_7\", \"description\": \"Radar reading from sensor radar sensor 7\"},\n",
    "#     {\"name\": \"package_type\", \"description\": \"Heartbeat indicates no significant change since last reading or change package type means that x, y or z has changed significantly +-30\"},\n",
    "#     {\"name\": \"f_cnt\", \"description\": \"number of packages transmitted since last network registration\"},\n",
    "#     {\"name\": \"dr\", \"description\": \"data rate parameter in LoRaWAN. It ranges between 1 and 5 where 1 is the slowest transmission data rate and 5 is the highest. This datarate is scaled by the network server depending on the signal quality of the past packages send\"},\n",
    "#     {\"name\": \"snr\", \"description\": \"signal to noise ratio – the higher value, the better the signal quality\"},\n",
    "#     {\"name\": \"rssi\", \"description\": \"signal strength – the higher value, the better the signal quality\"},\n",
    "#     {\"name\": \"psensor\", \"description\": \"sensor identifier (ex. EL1, EL2, EL3)\"},\n",
    "#     {\"name\": \"hw_fw_version\", \"description\": \"hardware and firmware version of the sensor\"},\n",
    "#     {\"name\": \"id\", \"description\": \"unique identifier for each datapoint made uuid4\"}\n",
    "# ]\n",
    "# \n",
    "# for desc in feature_descriptions: \n",
    "#     api_hist_bikelane_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next up:** 2: Latest API data\n",
    "Go to the 2_latest_api_feature_pipeline.ipynb notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
