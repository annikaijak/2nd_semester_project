{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backfill and Feature Engineering Notebook\n",
    "This notebook consists of 5 parts:\n",
    "1. Importing libaries and loading packages\n",
    "2. Data Loading\n",
    "3. Data Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Hopsworks Feature Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook, our decision-making process is informed by insights gained from the exploratory data analysis (EDA) we conducted. This analysis helped us identify the most relevant information for our methods and strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libaries and loading packages\n",
    "In this section, we import necessary libraries and define key functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Package for hopsworks\n",
    "# !pip install -U hopsworks --quiet\n",
    "\n",
    "#Packages\n",
    "import random\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "# For visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#UML\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "We load historic data to be used in the notebook.\n",
    "\n",
    "- The data provided by Sensade consists of 9 different CSV files. Loacated in the *data* folder.\n",
    "- The first three datasets are NORM1, 2 and 3. These CSV files consists of data from *normal* parking spots, but the data here is not collected in the same way as the data will be collected in the furture. therefore, it will not be used for our project.\n",
    "- Then theres FORB1, 2 and 3. These datasets are with data from places where parking is *forbidden* and will therefore not be suitable for our project.\n",
    "- The last three datasets EL1, 2 and 3 are datasets from electric car parking spots, collected the way data is collected in the furute and and will be used in the project for modelling.\n",
    "- More information on the specific dataset is found in the README.txt file in the *data* folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data as df_1, df_2, df_3\n",
    "df_1 = pd.read_csv('/workspaces/MLOps_Project/data/EL1.csv') # The path to the dataset should probably be changed when we're setting up the serverless-ml-pipeline\n",
    "df_2 = pd.read_csv('/workspaces/MLOps_Project/data/EL2.csv') # The path to the dataset should probably be changed when we're setting up the serverless-ml-pipeline\n",
    "df_3 = pd.read_csv('/workspaces/MLOps_Project/data/EL3.csv') # The path to the dataset should probably be changed when we're setting up the serverless-ml-pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "Now before just dumping the data into a feature store we do a little preprocessing to enhance the use of our datasets.\n",
    "\n",
    "This preprocessing consists of:\n",
    "- Making unique identifyers for each datapoint\n",
    "- Combining the three datasets into one \n",
    "- Making clusters used for labeling, which is nessesary when we want to train our models later\n",
    "- Converting the data column to pandas datetime\n",
    "- minor adjustments for the naming of radar columns to fix some hopsworks problem where the name of the columns cannot start with a number, and making the relevant columns into float format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique identifier for each row in the datasets\n",
    "def create_id(df, dataset_name):\n",
    "    # Assign the sensor prefix based on the dataset name\n",
    "    if dataset_name == 'df_1':\n",
    "        df['psensor'] = \"EL1\"\n",
    "    elif dataset_name == 'df_2':\n",
    "        df['psensor'] = \"EL2\"\n",
    "    elif dataset_name == 'df_3':\n",
    "        df['psensor'] = \"EL3\"\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset name provided\")\n",
    "\n",
    "    # Create a new column 'id' with a unique identifier for each row\n",
    "    df['id'] = [str(uuid.uuid4()) for _ in df.index]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to the datasets\n",
    "df_1 = create_id(df_1, 'df_1')\n",
    "df_2 = create_id(df_2, 'df_2')\n",
    "df_3 = create_id(df_3, 'df_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the datasets\n",
    "df_main = pd.concat([df_1, df_2, df_3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the date to datetime\n",
    "df_main['time'] = pd.to_datetime(df_main['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming the radar columns to start with radar\n",
    "df_main = df_main.rename(columns={'0_radar': 'radar_0', '1_radar': 'radar_1', '2_radar': 'radar_2', '3_radar': 'radar_3', '4_radar': 'radar_4', '5_radar': 'radar_5', '6_radar': 'radar_6', '7_radar': 'radar_7'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the columns to float\n",
    "df_main[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']] = df_main[['x','y','z', 'radar_0', 'radar_1', 'radar_2', 'radar_3', 'radar_4', 'radar_5', 'radar_6', 'radar_7', 'f_cnt', 'dr', 'rssi']].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "In this step, we develop a method to label the data points as either 'detection' or 'no_detection.' \n",
    "\n",
    "Our exploratory data analysis revealed that the electromagnetic field data is best suited for our objectives. Therefore, we focus on the x, y, and z data from this dataset.\n",
    "\n",
    "In our case, we chose KMeans as our clustering method and used the magnetic sensor data from the x, y, and z axes as features. This is done after normalizing the data using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dataframe for the features we wish to cluster on\n",
    "mag = df_main[[\"x\",\"y\",\"z\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "mag_normalized = scaler.fit_transform(mag)\n",
    "# Clustering the magnetic field data with 2 clusters using kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(mag_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding cluster labels to the mag dataframe\n",
    "mag = mag.copy() #dealing with an error\n",
    "mag['mag_cluster'] = kmeans.labels_\n",
    "df_label = df_main[['id', 'time']]\n",
    "df_label = df_label.copy() #dealing with an error\n",
    "df_label['mag_cluster'] = mag['mag_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label['mag_cluster'].replace({0: 'detection', 1: 'no_detection'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hopsworks Feature Storage\n",
    "\n",
    "Now we would like to connect to the Hopsworks Feature Store so we can access and create feature groups.\n",
    "\n",
    "In creating feature groups we take all the relevant coulmns and store it in hopworks, so that we later can acces and interperet for further use.\n",
    "\n",
    "We also specify a 'primary_key' that is used for relating diferent dimention tables to each other, in our case this is the unique ID that we made in the preprocessing step. \n",
    "\n",
    "The 'time' column is used as the event time key.\n",
    "\n",
    "we also we put `online_enabled` to `True` to make the feature group online for acces with an API when we make feature views.\n",
    "\n",
    "And finally we give descriptions to each coulmn with information given by the *README.txt* in *data*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/549014\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Connceting to the Hopsworks project\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature group for the magnetic field features\n",
    "mag_fg = fs.get_or_create_feature_group(\n",
    "    name=\"historic_parking_detection_features\",\n",
    "    version=1,\n",
    "    description=\"Historical data for parking detection\",\n",
    "    primary_key=['id'],\n",
    "    event_time='time',\n",
    "    online_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/549014/fs/544837/fg/766332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 20570/20570 | Elapsed Time: 00:08 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: historic_parking_detection_features_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/549014/jobs/named/historic_parking_detection_features_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7f274ba81e70>, None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the magnetic field features into the feature group\n",
    "mag_fg.insert(df_main, write_options={\"wait_for_job\" : False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making descriptions for the features\n",
    "feature_descriptions = [\n",
    "    {\"name\": \"time\", \"description\": \"Timepoint of the datapoint\"},\n",
    "    {\"name\": \"battery\", \"description\": \"Battery level of the sensor\"},\n",
    "    {\"name\": \"temperature\", \"description\": \"Temperature recorded by the sensor\"},\n",
    "    {\"name\": \"x\", \"description\": \"Magnetic field reading in the x direction\"},\n",
    "    {\"name\": \"y\", \"description\": \"Magnetic field reading in the y direction\"},\n",
    "    {\"name\": \"z\", \"description\": \"Magnetic field reading in the z direction\"},\n",
    "    {\"name\": \"radar_0\", \"description\": \"Radar reading from sensor radar sensor 0\"},\n",
    "    {\"name\": \"radar_1\", \"description\": \"Radar reading from sensor radar sensor 1\"},\n",
    "    {\"name\": \"radar_2\", \"description\": \"Radar reading from sensor radar sensor 2\"},\n",
    "    {\"name\": \"radar_3\", \"description\": \"Radar reading from sensor radar sensor 3\"},\n",
    "    {\"name\": \"radar_4\", \"description\": \"Radar reading from sensor radar sensor 4\"},\n",
    "    {\"name\": \"radar_5\", \"description\": \"Radar reading from sensor radar sensor 5\"},\n",
    "    {\"name\": \"radar_6\", \"description\": \"Radar reading from sensor radar sensor 6\"},\n",
    "    {\"name\": \"radar_7\", \"description\": \"Radar reading from sensor radar sensor 7\"},\n",
    "    {\"name\": \"package_type\", \"description\": \"Heartbeat indicates no significant change since last reading or change package type means that x, y or z has changed significantly +-30\"},\n",
    "    {\"name\": \"f_cnt\", \"description\": \"number of packages transmitted since last network registration\"},\n",
    "    {\"name\": \"dr\", \"description\": \"data rate parameter in LoRaWAN. It ranges between 1 and 5 where 1 is the slowest transmission data rate and 5 is the highest. This datarate is scaled by the network server depending on the signal quality of the past packages send\"},\n",
    "    {\"name\": \"snr\", \"description\": \"signal to noise ratio – the higher value, the better the signal quality\"},\n",
    "    {\"name\": \"rssi\", \"description\": \"signal strength – the higher value, the better the signal quality\"},\n",
    "    {\"name\": \"psensor\", \"description\": \"sensor identifier (ex. EL1, EL2, EL3)\"},\n",
    "    {\"name\": \"id\", \"description\": \"unique identifier for each datapoint made uuid4\"}\n",
    "]\n",
    "\n",
    "for desc in feature_descriptions: \n",
    "    mag_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature group for the magnetic field labels\n",
    "mag_label_fg = fs.get_or_create_feature_group(\n",
    "    name=\"historic_parking_detection_labels\",\n",
    "    version=1,\n",
    "    description=\"Historical labels on data for parking detection\",\n",
    "    primary_key=['id'],\n",
    "    event_time='time',\n",
    "    online_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/549014/fs/544837/fg/765325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 20570/20570 | Elapsed Time: 00:06 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: historic_parking_detection_labels_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/549014/jobs/named/historic_parking_detection_labels_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7f277442b7c0>, None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the magnetic field labels into the feature group\n",
    "mag_label_fg.insert(df_label, write_options={\"wait_for_job\" : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making descriptions for the labels\n",
    "feature_descriptions = [\n",
    "    {\"name\": \"time\", \"description\": \"Timepoint of the datapoint\"},\n",
    "    {\"name\": \"id\", \"description\": \"unique identifier for each datapoint made with uuid4\"},\n",
    "    {\"name\": \"mag_cluster\", \"description\": \"Label for the datapoint, whether a parking detection was made or not\"}\n",
    "]\n",
    "\n",
    "for desc in feature_descriptions: \n",
    "    mag_label_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next up:** 2: Feature Pipeline\n",
    "Go to the 2_feature_pipeline.ipynb notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
